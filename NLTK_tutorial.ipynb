{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I will be using the NLTK library for Natural Language Processing, credits Sentdex video tutorials on Youtube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First we will be discussing some methods which are used in Natural Language Processing for Preprocessing of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First Method: Tokenizing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be discussing two methods of tokenizing; sentence tokenizing and word tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "example_text = \"Hey man! Liverpool FC is playing tomorrow. Where are you watching it? I have a midterm tomorrow, have to prepare for it.\"\n",
    "\"\"\"Tokenising the text\"\"\"\n",
    "sentence_tokenized = sent_tokenize(example_text)\n",
    "word_tokenized = word_tokenize(example_text)\n",
    "\n",
    "for sentence in sentence_tokenized:\n",
    "    print(sentence)\n",
    "for word in word_tokenized:\n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second Method : Removing Stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stop words are words like \"The\", \"am\", \"is\" and others which appear in the text a lot more than the other words but don't help us in any way to understand the data more or provide any value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "filtered_words = []\n",
    "for word in word_tokenized:\n",
    "    if word not in stop_words:\n",
    "        filtered_words.append(word)\n",
    "\n",
    "print(filtered_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Third Method: Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stemming is like, it gives you the root of the word. It removes the \"ing\" or \"es\" or \"ess\" or \"s\" or other suffixes from the end of the word to give you the stem of the word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pstem = PorterStemmer()\n",
    "\n",
    "for word in word_tokenized:\n",
    "    print(pstem.stem(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like in the above example, you can see \"playing\" is stemmed to \"play\" and \"watching\" to \"watch\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another Example of Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w = ['cats', 'corns','caresses','carriers']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in w:\n",
    "    print(pstem.stem(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Third Method : Part of Speech Tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part of Speech or POS tagging is also called grammatical tagging, where the words of the text are marked or tagged according to the context and its meaning in accordance to the particular text it is used in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import state_union\n",
    "from nltk.tokenize import PunktSentenceTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_text = state_union.raw(\"1961-Kennedy.txt\")\n",
    "sample_text = state_union.raw(\"1962-Kennedy.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Training the PunktSentenceTokenizer\"\"\"\n",
    "custom_sent_tokenizer = PunktSentenceTokenizer(train_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenized = custom_sent_tokenizer.tokenize(sample_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processing_content():\n",
    "    try:\n",
    "        for sent in tokenized:\n",
    "            words = nltk.word_tokenize(sent)\n",
    "            tagged = nltk.pos_tag(words)\n",
    "            print(tagged)\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "\n",
    "processing_content()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The POS tagging gives you a tuple as an output, with the first element in the tuple the word in the text and the second element gives you POS tag of the first element, like whether it is a proposition, or a verb or a noun"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fourth method: Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def processing_content():\n",
    "    try:\n",
    "        for sent in tokenized:\n",
    "            words = nltk.word_tokenize(sent)\n",
    "            tagged = nltk.pos_tag(words)\n",
    "            \n",
    "            chunkGram = r\"\"\"Chunk: {<RB.?>*<VB.?>*<NNP>+<NN>?}\"\"\"\n",
    "            \n",
    "            chunkParser = nltk.RegexpParser(chunkGram)\n",
    "            chunked = chunkParser.parse(tagged)\n",
    "            \n",
    "            print(chunked)\n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "\n",
    "processing_content()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above you can see how the different words are chunked based on the POS tags, you can also draw these and see how they are chunked using the chunked.draw() method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Fifth Method : Named Entity Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processing_content():\n",
    "    try:\n",
    "        for sent in tokenized:\n",
    "            words = nltk.word_tokenize(sent)\n",
    "            tagged = nltk.pos_tag(words)\n",
    "            \n",
    "            named_entity = nltk.ne_chunk(tagged)\n",
    "            print(named_entity)\n",
    "            \n",
    "            \n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "\n",
    "processing_content()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Named Entity Recognition helps you information like whether the word in your text is a Person, an organisation, a location and much more. But the issue with NLTK's named entity recognition is that there is a high false positive rate, where a lot of misclassifications will be done and two words which should have been used as a chunk and then classified, are done separately. One of the parameters you can use with NLTK's named entity recognition is use:\n",
    "\n",
    "        nltk.ne_chunk(tagged, binary = True)\n",
    "\n",
    "This helps with chunking two words together and then doing their named entity recognition, but you are not able to see the tags assined to them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sixth Method: Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lemmatization is very similar to Stemming, but different!\n",
    "In this you get the word which shows the real meaning of the word that is being lemmatized. You can think of this as, you take a word, then look up its meaning in the dictionary and then replace it. The replaced word might be the same word or be completely different, but will carry the same meaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lemmatizer.lemmatize('better', pos = 'a'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lemmatizer.lemmatize('walks'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Finding synonyms and antonyms using NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "\"\"\"Finding synonyms of the word 'Good'.\"\"\"\n",
    "synonyms = wordnet.synsets(\"good\")\n",
    "print(synonyms)\n",
    "\"\"\"getting the first one only\"\"\"\n",
    "print(synonyms[0].lemmas()[0].name())\n",
    "\"\"\"getting the description and then example\"\"\"\n",
    "print(synonyms[0].definition())\n",
    "print(synonyms[0].examples())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding the similarity of the words used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In NLTK we can compare how similar two words are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word1 = wordnet.synset(\"apple.n.01\")\n",
    "word2 = wordnet.synset(\"orange.n.01\")\n",
    "print(word1.wup_similarity(word2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word1 = wordnet.synset(\"orange.n.01\")\n",
    "word2 = wordnet.synset(\"lemon.n.01\")\n",
    "print(word1.wup_similarity(word2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word1 = wordnet.synset(\"apple.n.01\")\n",
    "word2 = wordnet.synset(\"okra.n.01\")\n",
    "print(word1.wup_similarity(word2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word1 = wordnet.synset(\"apple.n.01\")\n",
    "word2 = wordnet.synset(\"pea.n.01\")\n",
    "print(word1.wup_similarity(word2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now if you had used something of totally different kind, like car or even chocolate might be, then you would have seen the similarity score go down."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we would be trying to classify the text based on the information we can get from the text itself and the tags associated with it. We cannot do this without having a tagged dataset, as we need the tags to train our classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import random\n",
    "from nltk.corpus import movie_reviews\n",
    "\n",
    "documents = [(list(movie_reviews.words(fileid)),category) for category in movie_reviews.categories() for fileid in movie_reviews.fileids(category)]\n",
    "\n",
    "random.shuffle(documents)\n",
    "all_words = []\n",
    "\n",
    "for word in movie_reviews.words():\n",
    "    all_words.append(word.lower())\n",
    "\n",
    "all_words = nltk.FreqDist(all_words)\n",
    "print(all_words.most_common(15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_features = list(all_words.keys())[:3000]\n",
    "\n",
    "def find_features(document):\n",
    "    words = set(document)\n",
    "    features = {}\n",
    "    for w in word_features:\n",
    "        features[w] = (w in words)\n",
    "    return features\n",
    "        \n",
    "print((find_features(movie_reviews.words('neg/cv000_29416.txt'))))        \n",
    "featuresets = [(find_features(rev),category) for (rev,category) in documents]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating our first classifier using the Naive Bayes Classifier of NLTK, the accuracy of this particular classifier is very unstable and therefore is not really reliable, sometimes it might go as high as 81.5%(as in my case), and the very next time you rearrange your data and run it, the accuracy might go around 55 to 60%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = featuresets[:1800]\n",
    "test_set = featuresets[1800:]\n",
    "\n",
    "classifier = nltk.NaiveBayesClassifier.train(training_set)\n",
    "print(\"Naive Bayes Algo accuracy percent:\", (nltk.classify.accuracy(classifier, test_set))*100)\n",
    "\n",
    "classifier.show_most_informative_features(15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = featuresets[:1900]\n",
    "test_set = featuresets[1900:]\n",
    "\n",
    "classifier = nltk.NaiveBayesClassifier.train(training_set)\n",
    "print(\"Naive Bayes Algo accuracy percent:\", (nltk.classify.accuracy(classifier, test_set))*100)\n",
    "\n",
    "classifier.show_most_informative_features(15)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK has a wrapper function using which we can call Scikitlearn's classifiers through NLTK only. Here we will be getting all the classifiers we can possibly think of, then build an ensemble out of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB, BernoulliNB\n",
    "training_set = featuresets[:1800]\n",
    "test_set = featuresets[1800:]\n",
    "print(\"Original Naive Bayes Algo accuracy percent:\", (nltk.classify.accuracy(classifier, test_set))*100)\n",
    "\n",
    "MNB_classifier = SklearnClassifier(MultinomialNB())\n",
    "MNB_classifier.train(training_set)\n",
    "print(\"MNB_classifier Naive Bayes Algo accuracy percent:\", (nltk.classify.accuracy(MNB_classifier, test_set))*100)\n",
    "\n",
    "#GaussianNB_classifier = SklearnClassifier(GaussianNB())\n",
    "#GaussianNB_classifier.train(training_set)\n",
    "#print(\"GaussianNB_classifier accuracy percent:\", (nltk.classify.accuracy(GaussianNB_classifier, test_set))*100)\n",
    "\n",
    "BernoulliNB_classifier = SklearnClassifier(BernoulliNB())\n",
    "BernoulliNB_classifier.train(training_set)\n",
    "print(\"BernoulliNB_classifier accuracy percent:\", (nltk.classify.accuracy(BernoulliNB_classifier, test_set))*100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.svm import SVC, LinearSVC, NuSVC\n",
    "\n",
    "LogisticRegression_classifier = SklearnClassifier(LogisticRegression())\n",
    "LogisticRegression_classifier.train(training_set)\n",
    "print(\"LogisticRegression_classifier accuracy percent:\", (nltk.classify.accuracy(LogisticRegression_classifier, test_set))*100)\n",
    "\n",
    "SGDClassifier_classifier = SklearnClassifier(SGDClassifier())\n",
    "SGDClassifier_classifier.train(training_set)\n",
    "print(\"SGDClassifier_classifier accuracy percent:\", (nltk.classify.accuracy(SGDClassifier_classifier, test_set))*100)\n",
    "\n",
    "SVC_classifier = SklearnClassifier(SVC())\n",
    "SVC_classifier.train(training_set)\n",
    "print(\"SVC_classifier accuracy percent:\", (nltk.classify.accuracy(SVC_classifier, test_set))*100)\n",
    "\n",
    "LinearSVC_classifier = SklearnClassifier(LinearSVC())\n",
    "LinearSVC_classifier.train(training_set)\n",
    "print(\"LinearSVC_classifier accuracy percent:\", (nltk.classify.accuracy(LinearSVC_classifier, test_set))*100)\n",
    "\n",
    "NuSVC_classifier = SklearnClassifier(NuSVC())\n",
    "NuSVC_classifier.train(training_set)\n",
    "print(\"NuSVC_classifier accuracy percent:\", (nltk.classify.accuracy(NuSVC_classifier, test_set))*100)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have started making our ensemble method, where this classifier will be taking votes from each and every classifier and outputing the one with the highest vote count and the related confidence, where the confidence is how many of the classifiers classified it in the same tag as the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.classify import ClassifierI\n",
    "from statistics import mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VoteClassifier(ClassifierI):\n",
    "    def __init__(self, *classifiers):\n",
    "        self._classifiers = classifiers\n",
    "        \n",
    "    def classify(self, features):\n",
    "        votes = []\n",
    "        for c in self._classifiers:\n",
    "            v = c.classify(features)\n",
    "            votes.append(v)\n",
    "        return mode(votes)\n",
    "    \n",
    "    def confidence(self, features):\n",
    "        votes = []\n",
    "        for c in self._classifiers:\n",
    "            v = c.classify(features)\n",
    "            votes.append(v)\n",
    "        choice_votes = votes.count(mode(votes))\n",
    "        conf = choice_votes / len(votes)\n",
    "        return conf\n",
    "    \n",
    "voted_classifier = VoteClassifier(classifier, MNB_classifier, BernoulliNB_classifier,\n",
    "                                 LogisticRegression_classifier,\n",
    "                                 SGDClassifier_classifier,LinearSVC_classifier,\n",
    "                                 NuSVC_classifier)\n",
    "\n",
    "print(\"voted_classifier accuracy percent:\", (nltk.classify.accuracy(voted_classifier, test_set))*100)\n",
    "\n",
    "print(\"Classification:\", voted_classifier.classify(test_set[0][0]),\"Confidence %:\", voted_classifier.confidence(test_set[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Classification:\", voted_classifier.classify(test_set[1][0]),\"Confidence %:\", voted_classifier.confidence(test_set[1][0])*100)\n",
    "\n",
    "print(\"Classification:\", voted_classifier.classify(test_set[2][0]),\"Confidence %:\", voted_classifier.confidence(test_set[2][0])*100)\n",
    "print(\"Classification:\", voted_classifier.classify(test_set[3][0]),\"Confidence %:\", voted_classifier.confidence(test_set[3][0])*100)\n",
    "print(\"Classification:\", voted_classifier.classify(test_set[4][0]),\"Confidence %:\", voted_classifier.confidence(test_set[4][0])*100)\n",
    "print(\"Classification:\", voted_classifier.classify(test_set[5][0]),\"Confidence %:\", voted_classifier.confidence(test_set[5][0])*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
